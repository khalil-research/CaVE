{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49b5424e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49b5424e",
    "outputId": "7ccfabbc-196d-4963-d0af-901eff1e872b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-Sklearn cannot be imported.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pyepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84efe8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CVXPY) Nov 09 12:22:42 PM: Encountered unexpected exception importing solver OSQP:\n",
      "ImportError('DLL load failed while importing qdldl: The specified module could not be found.')\n",
      "(CVXPY) Nov 09 12:22:42 PM: Encountered unexpected exception importing solver OSQP:\n",
      "ImportError('DLL load failed while importing qdldl: The specified module could not be found.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CLARABEL', 'COPT', 'ECOS', 'ECOS_BB', 'GUROBI', 'MOSEK', 'SCIPY', 'SCS']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cvxpy as cvx\n",
    "cvx.installed_solvers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e5be4e",
   "metadata": {
    "id": "53e5be4e"
   },
   "source": [
    "## Data Set and Optimization Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e3bd4d",
   "metadata": {
    "id": "00e3bd4d"
   },
   "outputs": [],
   "source": [
    "# generate data\n",
    "grid = (5,5) # grid size\n",
    "num_data = 1000 # number of training data\n",
    "num_feat = 5 # size of feature\n",
    "deg = 4 # polynomial degree\n",
    "e = 0 # noise width\n",
    "feats, costs = pyepo.data.shortestpath.genData(num_data+1000, num_feat, grid, deg, e, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6073c3f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6073c3f4",
    "outputId": "e5734002-11c7-44d6-a6ba-006b39d0d4e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-01-01\n",
      "Obj: 4.5703345390527765\n",
      "(0, 5)\n",
      "(5, 6)\n",
      "(6, 11)\n",
      "(11, 12)\n",
      "(12, 17)\n",
      "(17, 18)\n",
      "(18, 19)\n",
      "(19, 24)\n"
     ]
    }
   ],
   "source": [
    "from pyepo.model.grb import shortestPathModel\n",
    "# set solver\n",
    "optmodel = shortestPathModel(grid)\n",
    "# test\n",
    "optmodel.setObj(costs[0])\n",
    "sol, obj = optmodel.solve()\n",
    "print(\"Obj: {}\".format(obj))\n",
    "for i, e in enumerate(optmodel.arcs):\n",
    "    if sol[i] > 1e-3:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "976de0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, c_train, c_test = train_test_split(feats, costs, test_size=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac1e64dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac1e64dd",
    "outputId": "cfa534b0-e50f-4e05-ac72-374fe07eccf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing for optDataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:02<00:00, 386.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing for optDataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 711.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from dataset import optDatasetConstrs\n",
    "# get training and test data set\n",
    "dataset_train = optDatasetConstrs(optmodel, x_train, costs=c_train) # with binding constr\n",
    "dataset_test = pyepo.data.dataset.optDataset(optmodel, x_test, costs=c_test) # without binding constr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cfd1c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining constraints for optDataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 599.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# get training and test data set without costs\n",
    "dataset_train = optDatasetConstrs(optmodel, x_train, sols=dataset_train.sols) # with binding constr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77111dd8",
   "metadata": {
    "id": "77111dd8"
   },
   "outputs": [],
   "source": [
    "# get data loader\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 32\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a2da3",
   "metadata": {
    "id": "c21a2da3"
   },
   "source": [
    "## Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49603cb7",
   "metadata": {
    "id": "49603cb7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# build linear model\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(num_feat, (grid[0]-1)*grid[1]+(grid[1]-1)*grid[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71fb9a6",
   "metadata": {
    "id": "f71fb9a6"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f54ba370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train(reg, ca_cos, lr, num_epochs, log_step):\n",
    "    # set optimizer\n",
    "    optimizer = torch.optim.Adam(reg.parameters(), lr=lr)\n",
    "    # init log\n",
    "    loss_log, regret_log = [], [pyepo.metric.regret(reg, optmodel, loader_test)]\n",
    "    # running time\n",
    "    elapsed = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        tick = time.time()\n",
    "        for data in loader_train:\n",
    "            x, w, t_ctr = data\n",
    "            # forward pass\n",
    "            cp = reg(x)\n",
    "            loss = ca_cos(cp, t_ctr)\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_log.append(loss.item())\n",
    "        # record time\n",
    "        tock = time.time()\n",
    "        elapsed += tock - tick\n",
    "        if epoch % log_step == 0:\n",
    "            # regret\n",
    "            regret = pyepo.metric.regret(reg, optmodel, loader_test)\n",
    "            regret_log.append(regret)\n",
    "            print(\"Epoch {:3}, Loss: {:8.4f}, Regret: {:7.4f}%\".format(epoch, loss.item(), regret*100))\n",
    "    print(\"Elapsed Time: {:.2f} Sec\".format(elapsed))\n",
    "    return loss_log, regret_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363243b",
   "metadata": {},
   "source": [
    "### Gurobi Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e584e079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a3f0f20290>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b39f4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "from func import exactConeAlignedCosine\n",
    "\n",
    "class coneAlignedCosine(exactConeAlignedCosine):\n",
    "    def _getProjection(self, cp, ctr):\n",
    "        # ceate a model\n",
    "        m = gp.Model(\"projection\")\n",
    "        # turn off output\n",
    "        m.Params.outputFlag = 0\n",
    "        # varibles\n",
    "        p = m.addMVar(len(cp), name=\"x\", lb=-GRB.INFINITY)\n",
    "        λ = m.addMVar(len(ctr), name=\"λ\")\n",
    "        # onjective function\n",
    "        obj = (cp - p) @ (cp - p)\n",
    "        m.setObjective(obj, GRB.MINIMIZE)\n",
    "        # constraints\n",
    "        m.addConstr(ctr.T @ λ == p)\n",
    "        # focus on numeric problem\n",
    "        m.Params.NumericFocus = 3\n",
    "        # solve\n",
    "        m.optimize()\n",
    "        # get solutions\n",
    "        proj = np.array(p.X)\n",
    "        # normalize\n",
    "        proj = proj / np.linalg.norm(proj)\n",
    "        return torch.FloatTensor(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dd6a2e2",
   "metadata": {
    "id": "8dd6a2e2"
   },
   "outputs": [],
   "source": [
    "# init model\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67660513",
   "metadata": {
    "id": "67660513"
   },
   "outputs": [],
   "source": [
    "# init loss\n",
    "ca_cos = coneAlignedCosine(optmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c332205",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c332205",
    "outputId": "b01697bd-80f7-4f58-eb97-35b5276c0ff1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0, Loss:  -0.9972, Regret: 28.5068%\n",
      "Elapsed Time: 6.50 Sec\n"
     ]
    }
   ],
   "source": [
    "lr = 5e-3\n",
    "num_epochs = 1\n",
    "log_step = 1\n",
    "loss_log, regret_log = train(reg, ca_cos, lr, num_epochs, log_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b856df",
   "metadata": {},
   "source": [
    "## Gurobi with CVXPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "790ababa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a3f0f20290>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa18edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cvx\n",
    "\n",
    "from func import exactConeAlignedCosine\n",
    "\n",
    "class coneAlignedCosine(exactConeAlignedCosine):\n",
    "    def _getProjection(self, cp, ctr):\n",
    "        # varibles\n",
    "        p = cvx.Variable(len(cp), name=\"x\")\n",
    "        λ = cvx.Variable(len(ctr), name=\"λ\", nonneg=True)\n",
    "        # onjective function\n",
    "        objective = cvx.Minimize(cvx.sum_squares(cp - p))\n",
    "        # constraints\n",
    "        constraints = [ctr.T @ λ == p]\n",
    "        # ceate a model\n",
    "        problem = cvx.Problem(objective, constraints)\n",
    "        # solve and focus on numeric problem\n",
    "        problem.solve(solver=cvx.GUROBI, solver_opts={\"NumericFocus\": 3})\n",
    "        # get solutions\n",
    "        proj = p.value\n",
    "        # normalize\n",
    "        proj = proj / np.linalg.norm(proj)\n",
    "        return torch.FloatTensor(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e649843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acb19250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init loss\n",
    "ca_cos = coneAlignedCosine(optmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c7c98b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0, Loss:  -0.9972, Regret: 28.5068%\n",
      "Elapsed Time: 12.02 Sec\n"
     ]
    }
   ],
   "source": [
    "lr = 5e-3\n",
    "num_epochs = 1\n",
    "log_step = 1\n",
    "loss_log, regret_log = train(reg, ca_cos, lr, num_epochs, log_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fac41b",
   "metadata": {},
   "source": [
    "## COPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0ad966e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a3f0f20290>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6913a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coptpy import Envr, EnvrConfig\n",
    "from coptpy import COPT\n",
    "\n",
    "from func import exactConeAlignedCosine\n",
    "\n",
    "class coneAlignedCosine(exactConeAlignedCosine):\n",
    "    def _getProjection(self, cp, ctr):\n",
    "        # no banner\n",
    "        envconfig = EnvrConfig()\n",
    "        envconfig.set('nobanner', '1')\n",
    "        # ceate a model\n",
    "        m = Envr(envconfig).createModel(\"projection\")\n",
    "        # turn off output\n",
    "        m.setParam(\"Logging\", 0)\n",
    "        # varibles\n",
    "        p = m.addMVar(len(cp), nameprefix=\"x\", lb=-COPT.INFINITY)\n",
    "        λ = m.addMVar(len(ctr), nameprefix=\"λ\")\n",
    "        # onjective function\n",
    "        obj = (cp - p) @ (cp - p)\n",
    "        m.setObjective(obj, COPT.MINIMIZE)\n",
    "        # constraints\n",
    "        m.addConstr(ctr.T @ λ == p)\n",
    "        # focus on numeric problem\n",
    "        m.setParam(\"NumericFocus\", 3)\n",
    "        # solve\n",
    "        m.solve()\n",
    "        # get solutions\n",
    "        proj = np.array(p.X)\n",
    "        # normalize\n",
    "        proj = proj / np.linalg.norm(proj)\n",
    "        return torch.FloatTensor(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b839e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "145f2f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init loss\n",
    "ca_cos = coneAlignedCosine(optmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b58bd1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch   0, Loss:  -0.9972, Regret: 28.5068%\n",
      "Elapsed Time: 17.47 Sec\n"
     ]
    }
   ],
   "source": [
    "lr = 5e-3\n",
    "num_epochs = 1\n",
    "log_step = 1\n",
    "loss_log, regret_log = train(reg, ca_cos, lr, num_epochs, log_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50822ba",
   "metadata": {},
   "source": [
    "##  COPT with CVXPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a33cdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a3f0f20290>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5545f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cvx\n",
    "\n",
    "class coneAlignedCosine(exactConeAlignedCosine):\n",
    "    def _getProjection(self, cp, ctr):\n",
    "        # varibles\n",
    "        p = cvx.Variable(len(cp), name=\"x\")\n",
    "        λ = cvx.Variable(len(ctr), name=\"λ\", nonneg=True)\n",
    "        # onjective function\n",
    "        objective = cvx.Minimize(cvx.sum_squares(cp - p))\n",
    "        # constraints\n",
    "        constraints = [ctr.T @ λ == p]\n",
    "        # ceate a model\n",
    "        problem = cvx.Problem(objective, constraints)\n",
    "        # solve and focus on numeric problem\n",
    "        problem.solve(solver=cvx.COPT, NumericFocus=3)\n",
    "        # get solutions\n",
    "        proj = p.value\n",
    "        # normalize\n",
    "        proj = proj / np.linalg.norm(proj)\n",
    "        return torch.FloatTensor(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "559d1693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9651e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init loss\n",
    "ca_cos = coneAlignedCosine(optmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ebb8956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch   0, Loss:  -0.9972, Regret: 28.5068%\n",
      "Elapsed Time: 15.22 Sec\n"
     ]
    }
   ],
   "source": [
    "lr = 5e-3\n",
    "num_epochs = 1\n",
    "log_step = 1\n",
    "loss_log, regret_log = train(reg, ca_cos, lr, num_epochs, log_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f47cbd",
   "metadata": {},
   "source": [
    "## Mosek with CVXPY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04c4e518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a3f0f20290>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5dee8a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cvx\n",
    "\n",
    "from func import exactConeAlignedCosine\n",
    "\n",
    "class coneAlignedCosine(exactConeAlignedCosine):\n",
    "    def _getProjection(self, cp, ctr):\n",
    "        # varibles\n",
    "        p = cvx.Variable(len(cp), name=\"x\")\n",
    "        λ = cvx.Variable(len(ctr), name=\"λ\", nonneg=True)\n",
    "        # onjective function\n",
    "        objective = cvx.Minimize(cvx.sum_squares(cp - p))\n",
    "        # constraints\n",
    "        constraints = [ctr.T @ λ == p]\n",
    "        # ceate a model\n",
    "        problem = cvx.Problem(objective, constraints)\n",
    "        # solve and focus on numeric problem\n",
    "        problem.solve(solver=cvx.MOSEK)\n",
    "        # get solutions\n",
    "        proj = p.value\n",
    "        # normalize\n",
    "        proj = proj / np.linalg.norm(proj)\n",
    "        return torch.FloatTensor(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "427fc883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f8f40cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init loss\n",
    "ca_cos = coneAlignedCosine(optmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb8c53bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0, Loss:  -0.9972, Regret: 28.5068%\n",
      "Elapsed Time: 15.23 Sec\n"
     ]
    }
   ],
   "source": [
    "lr = 5e-3\n",
    "num_epochs = 1\n",
    "log_step = 1\n",
    "loss_log, regret_log = train(reg, ca_cos, lr, num_epochs, log_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358d183",
   "metadata": {},
   "source": [
    "##  Clarabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01effdfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a3f0f20290>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f3907fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clarabel as cl\n",
    "\n",
    "from func import exactConeAlignedCosine\n",
    "\n",
    "class ConeAlignedCosine(exactConeAlignedCosine):\n",
    "    def _getProjection(self, cp, ctr):\n",
    "        # number of variable\n",
    "        num_p = len(cp)\n",
    "        num_λ = ctr.shape[1]\n",
    "        # objective function: min ||cp - p||^2 => p^T * I * p - 2 * cp^T * p + cp.T * cp\n",
    "        # quadratic term\n",
    "        Q = np.zeros((num_p + num_λ, num_p + num_λ))\n",
    "        Q[:num_p, :num_p] = np.eye(num_p)\n",
    "        # linear term\n",
    "        p = np.zeros(num_p + num_λ)\n",
    "        p[:num_p] = - 2 * cp\n",
    "        # constraints\n",
    "        A = np.zeros((num_p + num_λ, num_p + num_λ))\n",
    "        b = np.zeros(num_p + num_λ)\n",
    "        # ctr.T @ λ == p =>  [-I, ctr.T] * [p, λ] == 0\n",
    "        A[:num_p] = np.hstack([-np.eye(num_p), ctr.T])\n",
    "        # λ >= 0\n",
    "        A[num_p:,num_p:] = - np.eye(num_λ)\n",
    "        # cone\n",
    "        cones = [cl.ZeroConeT(num_p), cl.NonnegativeConeT(num_λ)]\n",
    "        # settings\n",
    "        settings = cl.DefaultSettings()\n",
    "        settings.verbose = False\n",
    "        # QP model\n",
    "        model = cl.DefaultSolver(Q, p, A, b, cones, settings)\n",
    "        # solve\n",
    "        result = model.solve()\n",
    "        # get the solution\n",
    "        proj = result.x[:num_p]\n",
    "        # normalize\n",
    "        proj = proj / np.linalg.norm(proj)\n",
    "        return torch.FloatTensor(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4405e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "21ee110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init loss\n",
    "ca_cos = coneAlignedCosine(optmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "65882fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0, Loss:  -0.9972, Regret: 28.5068%\n",
      "Elapsed Time: 16.15 Sec\n"
     ]
    }
   ],
   "source": [
    "lr = 5e-3\n",
    "num_epochs = 1\n",
    "log_step = 1\n",
    "loss_log, regret_log = train(reg, ca_cos, lr, num_epochs, log_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7e4e9a",
   "metadata": {},
   "source": [
    "## Clarabel with CVXPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2072c170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a3f0f20290>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d437a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cvx\n",
    "\n",
    "from func import exactConeAlignedCosine\n",
    "\n",
    "class coneAlignedCosine(exactConeAlignedCosine):\n",
    "    def _getProjection(self, cp, ctr):\n",
    "        # varibles\n",
    "        p = cvx.Variable(len(cp), name=\"x\")\n",
    "        λ = cvx.Variable(len(ctr), name=\"λ\", nonneg=True)\n",
    "        # onjective function\n",
    "        objective = cvx.Minimize(cvx.sum_squares(cp - p))\n",
    "        # constraints\n",
    "        constraints = [ctr.T @ λ == p]\n",
    "        # ceate a model\n",
    "        problem = cvx.Problem(objective, constraints)\n",
    "        # solve and focus on numeric problem\n",
    "        problem.solve(solver=cvx.CLARABEL)\n",
    "        # get solutions\n",
    "        proj = p.value\n",
    "        # normalize\n",
    "        proj = proj / np.linalg.norm(proj)\n",
    "        return torch.FloatTensor(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4f46fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "038ef9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init loss\n",
    "ca_cos = coneAlignedCosine(optmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22167ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0, Loss:  -0.9972, Regret: 28.5068%\n",
      "Elapsed Time: 7.86 Sec\n"
     ]
    }
   ],
   "source": [
    "lr = 5e-3\n",
    "num_epochs = 1\n",
    "log_step = 1\n",
    "loss_log, regret_log = train(reg, ca_cos, lr, num_epochs, log_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a85c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dba1f36",
   "metadata": {},
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
