{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49b5424e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49b5424e",
    "outputId": "7ccfabbc-196d-4963-d0af-901eff1e872b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-Sklearn cannot be imported.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2093f104250>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pyepo\n",
    "\n",
    "# random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84efe8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CVXPY) Nov 08 11:19:56 PM: Encountered unexpected exception importing solver OSQP:\n",
      "ImportError('DLL load failed while importing qdldl: The specified module could not be found.')\n",
      "(CVXPY) Nov 08 11:19:57 PM: Encountered unexpected exception importing solver OSQP:\n",
      "ImportError('DLL load failed while importing qdldl: The specified module could not be found.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CLARABEL', 'COPT', 'ECOS', 'ECOS_BB', 'GUROBI', 'MOSEK', 'SCIPY', 'SCS']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cvxpy as cvx\n",
    "cvx.installed_solvers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e5be4e",
   "metadata": {
    "id": "53e5be4e"
   },
   "source": [
    "## Data Set and Optimization Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e3bd4d",
   "metadata": {
    "id": "00e3bd4d"
   },
   "outputs": [],
   "source": [
    "# generate data\n",
    "grid = (5,5) # grid size\n",
    "num_data = 1000 # number of training data\n",
    "num_feat = 5 # size of feature\n",
    "deg = 4 # polynomial degree\n",
    "e = 0 # noise width\n",
    "feats, costs = pyepo.data.shortestpath.genData(num_data+1000, num_feat, grid, deg, e, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6073c3f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6073c3f4",
    "outputId": "e5734002-11c7-44d6-a6ba-006b39d0d4e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-01-01\n",
      "Obj: 4.5703345390527765\n",
      "(0, 5)\n",
      "(5, 6)\n",
      "(6, 11)\n",
      "(11, 12)\n",
      "(12, 17)\n",
      "(17, 18)\n",
      "(18, 19)\n",
      "(19, 24)\n"
     ]
    }
   ],
   "source": [
    "from pyepo.model.grb import shortestPathModel\n",
    "# set solver\n",
    "optmodel = shortestPathModel(grid)\n",
    "# test\n",
    "optmodel.setObj(costs[0])\n",
    "sol, obj = optmodel.solve()\n",
    "print(\"Obj: {}\".format(obj))\n",
    "for i, e in enumerate(optmodel.arcs):\n",
    "    if sol[i] > 1e-3:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "976de0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, c_train, c_test = train_test_split(feats, costs, test_size=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac1e64dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac1e64dd",
    "outputId": "cfa534b0-e50f-4e05-ac72-374fe07eccf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing for optDataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:02<00:00, 337.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing for optDataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 535.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from dataset import optDatasetConstrs\n",
    "# get training and test data set\n",
    "dataset_train = optDatasetConstrs(optmodel, x_train, costs=c_train) # with binding constr\n",
    "dataset_test = pyepo.data.dataset.optDataset(optmodel, x_test, costs=c_test) # without binding constr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cfd1c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining constraints for optDataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:02<00:00, 493.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# get training and test data set without costs\n",
    "dataset_train = optDatasetConstrs(optmodel, x_train, sols=dataset_train.sols) # with binding constr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77111dd8",
   "metadata": {
    "id": "77111dd8"
   },
   "outputs": [],
   "source": [
    "# get data loader\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 32\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a2da3",
   "metadata": {
    "id": "c21a2da3"
   },
   "source": [
    "## Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49603cb7",
   "metadata": {
    "id": "49603cb7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# build linear model\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(num_feat, (grid[0]-1)*grid[1]+(grid[1]-1)*grid[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71fb9a6",
   "metadata": {
    "id": "f71fb9a6"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f54ba370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train(reg, ca_cos, lr, num_epochs, log_step):\n",
    "    # set optimizer\n",
    "    optimizer = torch.optim.Adam(reg.parameters(), lr=lr)\n",
    "    # init log\n",
    "    loss_log, regret_log = [], [pyepo.metric.regret(reg, optmodel, loader_test)]\n",
    "    # running time\n",
    "    elapsed = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        tick = time.time()\n",
    "        for data in loader_train:\n",
    "            x, w, t_ctr = data\n",
    "            # forward pass\n",
    "            cp = reg(x)\n",
    "            loss = ca_cos(cp, t_ctr)\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_log.append(loss.item())\n",
    "        # record time\n",
    "        tock = time.time()\n",
    "        elapsed += tock - tick\n",
    "        if epoch % log_step == 0:\n",
    "            # regret\n",
    "            regret = pyepo.metric.regret(reg, optmodel, loader_test)\n",
    "            regret_log.append(regret)\n",
    "            print(\"Epoch {:3}, Loss: {:8.4f}, Regret: {:7.4f}%\".format(epoch, loss.item(), regret*100))\n",
    "    print(\"Elapsed Time: {:.2f} Sec\".format(elapsed))\n",
    "    return loss_log, regret_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363243b",
   "metadata": {},
   "source": [
    "### Gurobi Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b39f4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "from func import exactConeAlignedCosine\n",
    "\n",
    "class coneAlignedCosine(exactConeAlignedCosine):\n",
    "    def _getProjection(self, cp, ctr):\n",
    "        # ceate a model\n",
    "        m = gp.Model(\"projection\")\n",
    "        # turn off output\n",
    "        m.Params.outputFlag = 0\n",
    "        # varibles\n",
    "        p = m.addMVar(len(cp), name=\"x\", lb=-GRB.INFINITY)\n",
    "        λ = m.addMVar(len(ctr), name=\"λ\")\n",
    "        # onjective function\n",
    "        obj = (cp - p) @ (cp - p)\n",
    "        m.setObjective(obj, GRB.MINIMIZE)\n",
    "        # constraints\n",
    "        m.addConstr(ctr.T @ λ == p)\n",
    "        # focus on numeric problem\n",
    "        m.Params.NumericFocus = 3\n",
    "        # solve\n",
    "        m.optimize()\n",
    "        # get solutions\n",
    "        proj = np.array(p.X)\n",
    "        # normalize\n",
    "        proj = proj / np.linalg.norm(proj)\n",
    "        return torch.FloatTensor(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dd6a2e2",
   "metadata": {
    "id": "8dd6a2e2"
   },
   "outputs": [],
   "source": [
    "# init model\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67660513",
   "metadata": {
    "id": "67660513"
   },
   "outputs": [],
   "source": [
    "# init loss\n",
    "ca_cos = coneAlignedCosine(optmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c332205",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c332205",
    "outputId": "b01697bd-80f7-4f58-eb97-35b5276c0ff1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0, Loss:  -0.9972, Regret: 28.5068%\n",
      "Elapsed Time: 6.61 Sec\n"
     ]
    }
   ],
   "source": [
    "lr = 5e-3\n",
    "num_epochs = 1\n",
    "log_step = 1\n",
    "loss_log, regret_log = train(reg, ca_cos, lr, num_epochs, log_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b856df",
   "metadata": {},
   "source": [
    "## Gurobi with CVXPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa18edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cvx\n",
    "\n",
    "from func import exactConeAlignedCosine\n",
    "\n",
    "class coneAlignedCosine(exactConeAlignedCosine):\n",
    "    def _getProjection(self, cp, ctr):\n",
    "        # varibles\n",
    "        p = cvx.Variable(len(cp), name=\"x\")\n",
    "        λ = cvx.Variable(len(ctr), name=\"λ\", nonneg=True)\n",
    "        # onjective function\n",
    "        objective = cvx.Minimize(cvx.sum_squares(cp - p))\n",
    "        # constraints\n",
    "        constraints = [ctr.T @ λ == p]\n",
    "        # ceate a model\n",
    "        problem = cvx.Problem(objective, constraints)\n",
    "        # solve and focus on numeric problem\n",
    "        problem.solve(solver=cvx.GUROBI, solver_opts={\"NumericFocus\": 3})\n",
    "        # get solutions\n",
    "        proj = p.value\n",
    "        # normalize\n",
    "        proj = proj / np.linalg.norm(proj)\n",
    "        return torch.FloatTensor(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e649843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acb19250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init loss\n",
    "ca_cos = coneAlignedCosine(optmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c7c98b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0, Loss:  -0.9949, Regret: 23.2719%\n",
      "Elapsed Time: 12.40 Sec\n"
     ]
    }
   ],
   "source": [
    "lr = 5e-3\n",
    "num_epochs = 1\n",
    "log_step = 1\n",
    "loss_log, regret_log = train(reg, ca_cos, lr, num_epochs, log_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fac41b",
   "metadata": {},
   "source": [
    "## COPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6913a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coptpy import Envr, EnvrConfig\n",
    "from coptpy import COPT\n",
    "\n",
    "from func import exactConeAlignedCosine\n",
    "\n",
    "class coneAlignedCosine(exactConeAlignedCosine):\n",
    "    def _getProjection(self, cp, ctr):\n",
    "        # no banner\n",
    "        envconfig = EnvrConfig()\n",
    "        envconfig.set('nobanner', '1')\n",
    "        # ceate a model\n",
    "        m = Envr(envconfig).createModel(\"projection\")\n",
    "        # turn off output\n",
    "        m.setParam(\"Logging\", 0)\n",
    "        # varibles\n",
    "        p = m.addMVar(len(cp), nameprefix=\"x\", lb=-COPT.INFINITY)\n",
    "        λ = m.addMVar(len(ctr), nameprefix=\"λ\")\n",
    "        # onjective function\n",
    "        obj = (cp - p) @ (cp - p)\n",
    "        m.setObjective(obj, COPT.MINIMIZE)\n",
    "        # constraints\n",
    "        m.addConstr(ctr.T @ λ == p)\n",
    "        # focus on numeric problem\n",
    "        m.setParam(\"NumericFocus\", 3)\n",
    "        # solve\n",
    "        m.solve()\n",
    "        # get solutions\n",
    "        proj = np.array(p.X)\n",
    "        # normalize\n",
    "        proj = proj / np.linalg.norm(proj)\n",
    "        return torch.FloatTensor(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b839e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "145f2f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init loss\n",
    "ca_cos = coneAlignedCosine(optmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b58bd1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch   0, Loss:  -0.9968, Regret: 17.0196%\n",
      "Elapsed Time: 17.28 Sec\n"
     ]
    }
   ],
   "source": [
    "lr = 5e-3\n",
    "num_epochs = 1\n",
    "log_step = 1\n",
    "loss_log, regret_log = train(reg, ca_cos, lr, num_epochs, log_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50822ba",
   "metadata": {},
   "source": [
    "##  COPT with CVXPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5545f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cvx\n",
    "\n",
    "class coneAlignedCosine(exactConeAlignedCosine):\n",
    "    def _getProjection(self, cp, ctr):\n",
    "        # varibles\n",
    "        p = cvx.Variable(len(cp), name=\"x\")\n",
    "        λ = cvx.Variable(len(ctr), name=\"λ\", nonneg=True)\n",
    "        # onjective function\n",
    "        objective = cvx.Minimize(cvx.sum_squares(cp - p))\n",
    "        # constraints\n",
    "        constraints = [ctr.T @ λ == p]\n",
    "        # ceate a model\n",
    "        problem = cvx.Problem(objective, constraints)\n",
    "        # solve and focus on numeric problem\n",
    "        problem.solve(solver=cvx.COPT, NumericFocus=3)\n",
    "        # get solutions\n",
    "        proj = p.value\n",
    "        # normalize\n",
    "        proj = proj / np.linalg.norm(proj)\n",
    "        return torch.FloatTensor(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "559d1693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9651e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init loss\n",
    "ca_cos = coneAlignedCosine(optmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ebb8956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch   0, Loss:  -0.9921, Regret: 16.5421%\n",
      "Elapsed Time: 14.81 Sec\n"
     ]
    }
   ],
   "source": [
    "lr = 5e-3\n",
    "num_epochs = 1\n",
    "log_step = 1\n",
    "loss_log, regret_log = train(reg, ca_cos, lr, num_epochs, log_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f47cbd",
   "metadata": {},
   "source": [
    "## Mosek with CVXPY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dee8a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cvx\n",
    "\n",
    "from func import exactConeAlignedCosine\n",
    "\n",
    "class coneAlignedCosine(exactConeAlignedCosine):\n",
    "    def _getProjection(self, cp, ctr):\n",
    "        # varibles\n",
    "        p = cvx.Variable(len(cp), name=\"x\")\n",
    "        λ = cvx.Variable(len(ctr), name=\"λ\", nonneg=True)\n",
    "        # onjective function\n",
    "        objective = cvx.Minimize(cvx.sum_squares(cp - p))\n",
    "        # constraints\n",
    "        constraints = [ctr.T @ λ == p]\n",
    "        # ceate a model\n",
    "        problem = cvx.Problem(objective, constraints)\n",
    "        # solve and focus on numeric problem\n",
    "        problem.solve(solver=cvx.MOSEK)\n",
    "        # get solutions\n",
    "        proj = p.value\n",
    "        # normalize\n",
    "        proj = proj / np.linalg.norm(proj)\n",
    "        return torch.FloatTensor(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "427fc883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f8f40cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init loss\n",
    "ca_cos = coneAlignedCosine(optmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb8c53bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0, Loss:  -0.9966, Regret: 22.1378%\n",
      "Elapsed Time: 15.68 Sec\n"
     ]
    }
   ],
   "source": [
    "lr = 5e-3\n",
    "num_epochs = 1\n",
    "log_step = 1\n",
    "loss_log, regret_log = train(reg, ca_cos, lr, num_epochs, log_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358d183",
   "metadata": {},
   "source": [
    "##  Clarabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9191fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clarabel as cl\n",
    "from scipy import sparse\n",
    "\n",
    "from func import exactConeAlignedCosine\n",
    "\n",
    "class coneAlignedCosine(exactConeAlignedCosine):\n",
    "    def _getProjection(self, cp, ctr):\n",
    "        # number of variable\n",
    "        num_p = len(cp)\n",
    "        num_λ = len(ctr)\n",
    "        # objective function: min ||cp - p||^2 => p^T * I * p - 2 * cp^T * p + cp.T * cp\n",
    "        # quadratic term\n",
    "        Q = np.zeros((num_p+num_λ, num_p+num_λ))\n",
    "        Q[:num_p, :num_p] = np.eye(num_p)\n",
    "        Q = sparse.csc_matrix(Q)\n",
    "        # linear term\n",
    "        p = np.zeros(num_p+num_λ)\n",
    "        p[:num_p] = - 2 * cp\n",
    "        # constraints\n",
    "        A = np.zeros((num_p+num_λ, num_p+num_λ))\n",
    "        b = sparse.csc_matrix(np.zeros(num_p+num_λ))\n",
    "        # ctr.T @ λ == p =>  [-I, ctr.T] * [p, λ] == 0\n",
    "        A[:num_p] = np.hstack([-np.eye(num_p), ctr.T])\n",
    "        # λ >= 0\n",
    "        A[num_p:,num_p:] = - np.eye(num_λ)\n",
    "        # cone\n",
    "        cones = [cl.ZeroConeT(num_p), cl.NonnegativeConeT(num_λ)]\n",
    "        # settings\n",
    "        settings = cl.DefaultSettings()\n",
    "        settings.verbose = False\n",
    "        # QP model\n",
    "        model = cl.DefaultSolver(Q, p, A, b, cones, settings)\n",
    "        # solve\n",
    "        result = model.solve()\n",
    "        # get the solution\n",
    "        proj = result.x[:num_p]\n",
    "        # normalize\n",
    "        proj = proj / np.linalg.norm(proj)\n",
    "        return torch.FloatTensor(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4405e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21ee110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init loss\n",
    "ca_cos = coneAlignedCosine(optmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65882fd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "multi-dimensional sub-views are not implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      3\u001b[0m log_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 4\u001b[0m loss_log, regret_log \u001b[38;5;241m=\u001b[39m train(reg, ca_cos, lr, num_epochs, log_step)\n",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(reg, ca_cos, lr, num_epochs, log_step)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m     14\u001b[0m cp \u001b[38;5;241m=\u001b[39m reg(x)\n\u001b[1;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m ca_cos(cp, t_ctr)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cvx\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cvx\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\OneDrive\\Study\\UofT\\Research\\CAVE\\func.py:37\u001b[0m, in \u001b[0;36mexactConeAlignedCosine.forward\u001b[1;34m(self, pred_cost, tight_ctrs, reduction)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pred_cost, tight_ctrs, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Forward pass\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calLoss(pred_cost, tight_ctrs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptmodel)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# reduction\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mC:\\OneDrive\\Study\\UofT\\Research\\CAVE\\func.py:68\u001b[0m, in \u001b[0;36mexactConeAlignedCosine._calLoss\u001b[1;34m(self, pred_cost, tight_ctrs, optmodel)\u001b[0m\n\u001b[0;32m     65\u001b[0m ctrs \u001b[38;5;241m=\u001b[39m tight_ctrs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# get projection\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getProjection(cp[i], np\u001b[38;5;241m.\u001b[39munique(ctrs[i], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# calculate cosine similarity\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     loss[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m F\u001b[38;5;241m.\u001b[39mcosine_similarity(pred_cost[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m     71\u001b[0m                                     p\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "Cell \u001b[1;32mIn[31], line 32\u001b[0m, in \u001b[0;36mconeAlignedCosine._getProjection\u001b[1;34m(self, cp, ctr)\u001b[0m\n\u001b[0;32m     30\u001b[0m settings\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# QP model\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m model \u001b[38;5;241m=\u001b[39m cl\u001b[38;5;241m.\u001b[39mDefaultSolver(Q, p, A, b, cones, settings)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# solve\u001b[39;00m\n\u001b[0;32m     34\u001b[0m result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39msolve()\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: multi-dimensional sub-views are not implemented"
     ]
    }
   ],
   "source": [
    "lr = 5e-3\n",
    "num_epochs = 1\n",
    "log_step = 1\n",
    "loss_log, regret_log = train(reg, ca_cos, lr, num_epochs, log_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7e4e9a",
   "metadata": {},
   "source": [
    "## Clarabel with CVXPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d437a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cvx\n",
    "\n",
    "from func import exactConeAlignedCosine\n",
    "\n",
    "class coneAlignedCosine(exactConeAlignedCosine):\n",
    "    def _getProjection(self, cp, ctr):\n",
    "        # varibles\n",
    "        p = cvx.Variable(len(cp), name=\"x\")\n",
    "        λ = cvx.Variable(len(ctr), name=\"λ\", nonneg=True)\n",
    "        # onjective function\n",
    "        objective = cvx.Minimize(cvx.sum_squares(cp - p))\n",
    "        # constraints\n",
    "        constraints = [ctr.T @ λ == p]\n",
    "        # ceate a model\n",
    "        problem = cvx.Problem(objective, constraints)\n",
    "        # solve and focus on numeric problem\n",
    "        problem.solve(solver=cvx.CLARABEL)\n",
    "        # get solutions\n",
    "        proj = p.value\n",
    "        # normalize\n",
    "        proj = proj / np.linalg.norm(proj)\n",
    "        return torch.FloatTensor(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f46fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "038ef9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init loss\n",
    "ca_cos = coneAlignedCosine(optmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22167ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0, Loss:  -0.9936, Regret: 14.7692%\n",
      "Elapsed Time: 7.93 Sec\n"
     ]
    }
   ],
   "source": [
    "lr = 5e-3\n",
    "num_epochs = 1\n",
    "log_step = 1\n",
    "loss_log, regret_log = train(reg, ca_cos, lr, num_epochs, log_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a85c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dba1f36",
   "metadata": {},
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
